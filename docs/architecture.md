# DocVault - System Architecture

> **Last Updated:** 2026-02-12
> **Status:** M4 Completed - Document Parsers Working

---

## Table of Contents

1. [Overview](#overview)
2. [System Architecture](#system-architecture)
3. [Layer-by-Layer Breakdown](#layer-by-layer-breakdown)
4. [Data Flow](#data-flow)
5. [Key Design Decisions](#key-design-decisions)
6. [Technology Stack](#technology-stack)
7. [Scalability Considerations](#scalability-considerations)

---

## Overview

DocVault is a **local-first RAG (Retrieval-Augmented Generation)** system designed for querying enterprise documentation across multiple projects. The system converts documents into searchable vector representations and uses AI to answer questions based on the indexed content.

### Core Principles

1. **Local-First**: Works 100% locally without external API dependencies
2. **Flexible Scalability**: Easy migration to commercial models or own server
3. **Multi-Project Support**: Handle documentation from multiple projects with context preservation
4. **Multi-Format**: Support PDFs, HTML, and Markdown documents
5. **Incremental Development**: Built milestone by milestone with verification at each step

---

## System Architecture

### High-Level Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         APPLICATION LAYER                               â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   FastAPI       â”‚  â”‚   Streamlit     â”‚  â”‚   CLI Tool      â”‚      â”‚
â”‚  â”‚   REST API      â”‚  â”‚   Web UI        â”‚  â”‚   Terminal      â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                      â”‚                      â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RAG PIPELINE LAYER                              â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Query Processing â†’ Retrieval â†’ Context Assembly â†’ Generation â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                             â”‚
            â–¼                                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     LLM PROVIDER LAYER          â”‚    â”‚     VECTOR SEARCH LAYER         â”‚
â”‚                                 â”‚    â”‚                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   LLM Factory          â”‚    â”‚    â”‚  â”‚   Qdrant Client        â”‚    â”‚
â”‚  â”‚   (Strategy Pattern)   â”‚    â”‚    â”‚  â”‚   (Vector Search)      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚           â”‚                     â”‚    â”‚           â”‚                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ â€¢ Ollama Local         â”‚    â”‚    â”‚  â”‚ â€¢ Similarity Search    â”‚    â”‚
â”‚  â”‚ â€¢ Ollama Server        â”‚    â”‚    â”‚  â”‚ â€¢ Metadata Filtering   â”‚    â”‚
â”‚  â”‚ â€¢ OpenAI API           â”‚    â”‚    â”‚  â”‚ â€¢ Collection Mgmt      â”‚    â”‚
â”‚  â”‚ â€¢ Anthropic API        â”‚    â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
                                                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         EMBEDDING LAYER                                 â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   EmbeddingService (sentence-transformers)                     â”‚   â”‚
â”‚  â”‚   â€¢ Model: paraphrase-multilingual-MiniLM-L12-v2               â”‚   â”‚
â”‚  â”‚   â€¢ Dimension: 384                                              â”‚   â”‚
â”‚  â”‚   â€¢ Languages: English + Spanish                                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â–²
                                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DOCUMENT PROCESSING LAYER                          â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚ PDF Parser   â”‚  â”‚ HTML Parser  â”‚  â”‚  MD Parser   â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚         â”‚                  â”‚                  â”‚                         â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                            â”‚                                            â”‚
â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚                 â”‚  Text Extraction    â”‚                                â”‚
â”‚                 â”‚  & Preprocessing    â”‚                                â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                            â”‚                                            â”‚
â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚                 â”‚  Chunking Strategy  â”‚                                â”‚
â”‚                 â”‚  (~500 tokens)      â”‚                                â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DATA STORAGE LAYER                              â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚   Qdrant Storage     â”‚         â”‚   Document Store     â”‚            â”‚
â”‚  â”‚   (Vectors)          â”‚         â”‚   (Original Files)   â”‚            â”‚
â”‚  â”‚  ./data/qdrant/      â”‚         â”‚  ./data/documents/   â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Layer-by-Layer Breakdown

### 1. Application Layer (M7)

**Purpose:** User-facing interfaces

**Components:**
- **FastAPI REST API:** Programmatic access to RAG functionality
- **Streamlit Web UI:** Interactive web interface for querying
- **CLI Tool:** Terminal-based query interface

**Key Features:**
- Authentication & authorization
- Request validation
- Response formatting
- Error handling

---

### 2. RAG Pipeline Layer (M7)

**Purpose:** Orchestrate the retrieval and generation process

**Workflow:**
```
User Query â†’ Query Processing â†’ Vector Search â†’ Context Assembly â†’ LLM Generation â†’ Response
```

**Components:**
- **Query Processor:** Clean and prepare user queries
- **Retrieval Engine:** Fetch relevant documents from Qdrant
- **Context Assembler:** Build context from retrieved chunks
- **Response Generator:** Generate answer using LLM + context

---

### 3. LLM Provider Layer (M6)

**Purpose:** Flexible LLM abstraction using Strategy Pattern

**Architecture:**
```python
# Interface
class LLMProvider:
    def generate(self, prompt: str, context: str) -> str
    def is_available(self) -> bool

# Implementations
- OllamaLocalProvider    # localhost:11434
- OllamaServerProvider   # Custom server URL
- OpenAIProvider         # OpenAI API
- AnthropicProvider      # Claude API

# Factory
LLMFactory.create(provider_type) â†’ LLMProvider
```

**Key Design Decision:**
- **No vendor lock-in:** Switch providers with a single config change
- **Automatic fallback:** Try local â†’ server â†’ API
- **Cost flexibility:** Start free (local), scale as needed

---

### 4. Vector Search Layer (M3) âœ…

**Purpose:** Fast semantic similarity search

**Technology:** Qdrant Vector Database

**Capabilities:**
- **HNSW Index:** Fast approximate nearest neighbor search
- **Metadata Filtering:** Filter by project, date, type, etc.
- **Collection Management:** Separate collections per project
- **Persistence:** Local storage in `./data/qdrant_storage/`

**Search Process:**
```
Query Text â†’ Embedding (384 dims) â†’ Qdrant Search (Top-K) â†’ Relevant Chunks
```

**Performance:**
- Search latency: ~10-50ms
- Throughput: ~1000 queries/sec
- Storage: ~1KB per document chunk

---

### 5. Embedding Layer (M2) âœ…

**Purpose:** Convert text to vector representations

**Technology:** sentence-transformers

**Model:** `paraphrase-multilingual-MiniLM-L12-v2`
- **Dimensions:** 384
- **Languages:** English + Spanish
- **Size:** ~120MB
- **Speed:** ~50ms per embedding (CPU)

**Key Features:**
- **Batch processing:** Process multiple texts efficiently
- **L2 normalization:** Optimized for cosine similarity
- **Local execution:** No API calls, fully private
- **Caching:** Model cached after first load

**API:**
```python
service = EmbeddingService()

# Single text
embedding = service.generate_embedding("Hello world")
# â†’ [0.123, -0.456, ..., 0.789]  # 384 floats

# Batch (efficient)
embeddings = service.generate_batch_embeddings(
    ["text1", "text2", "text3"],
    batch_size=32
)
```

---

### 6. Document Processing Layer (M4-M5)

**Purpose:** Extract and prepare text from various document formats

**Components:**

#### M4: Parsers âœ…
- **PDF Parser:** Extract text from PDFs (pypdf) â€” page-by-page extraction with metadata
- **HTML Parser:** Extract content from web pages (BeautifulSoup4 + lxml) â€” boilerplate removal
- **Markdown Parser:** Parse Markdown documents (python-frontmatter) â€” YAML frontmatter support
- **Parser Factory:** Automatic format detection by file extension
- **ParsedDocument:** Standard output dataclass for all parsers

**Features:**
- Preserve document structure (headings, sections)
- Extract metadata (title, author, date, page count)
- Handle special characters and encodings
- Remove HTML boilerplate (scripts, nav, sidebar, ads)
- YAML frontmatter extraction for Markdown

#### M5: Ingestion Pipeline
- **Text Chunking:** Split documents into ~500 token chunks
- **Overlap Strategy:** 50 tokens overlap between chunks
- **Metadata Enrichment:** Add filename, chunk_id, project, etc.
- **Batch Processing:** Process entire folders efficiently

**Chunking Strategy:**
```
Document (10,000 tokens)
    â†“
Chunk 1: tokens   0-500  (50 overlap) â†’ Embedding 1
Chunk 2: tokens 450-950  (50 overlap) â†’ Embedding 2
Chunk 3: tokens 900-1400 (50 overlap) â†’ Embedding 3
...
```

**Why overlap?**
- Prevent context loss at chunk boundaries
- Improve retrieval of information spanning multiple chunks

---

### 7. Data Storage Layer (M3-M5)

**Purpose:** Persist documents and vectors

**Components:**

1. **Qdrant Vector Storage** (M3)
   - Location: `./data/qdrant_storage/`
   - Format: Optimized binary format (HNSW index)
   - Content: Embeddings (384 dims) + metadata
   - Gitignored: Can be regenerated from documents

2. **Document Storage** (M4-M5)
   - Location: `./data/documents/`
   - Format: Original files (PDF, HTML, MD)
   - Structure: Organized by project
   - Versioned: Keep original documents in git (if small)

---

## Data Flow

### Ingestion Flow (Documents â†’ Vector DB)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PDF/HTML/MD     â”‚ User places documents in data/documents/
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [M4] Parser     â”‚ Extract text based on file type
â”‚ â€¢ PDFParser     â”‚
â”‚ â€¢ HTMLParser    â”‚
â”‚ â€¢ MDParser      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [M5] Chunker    â”‚ Split into ~500 token chunks with 50 token overlap
â”‚ â€¢ Tokenization  â”‚
â”‚ â€¢ Overlap       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [M2] Embeddings â”‚ Generate 384-dim vectors
â”‚ EmbeddingServiceâ”‚ Batch processing for efficiency
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [M3] Qdrant     â”‚ Store vectors + metadata
â”‚ Insert vectors  â”‚ Create HNSW index
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Example:**
```python
# M5: Ingestion script
python scripts/ingest_documents.py data/documents/project1/ --collection "project1"

# Process:
1. Find all PDF/HTML/MD files
2. Parse each file â†’ extract text
3. Chunk text â†’ [chunk1, chunk2, ..., chunkN]
4. Generate embeddings â†’ [emb1, emb2, ..., embN]
5. Insert to Qdrant with metadata
```

---

### Query Flow (User Question â†’ Answer)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Query      â”‚ "How to install dependencies?"
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [M2] Embeddings â”‚ Convert query to 384-dim vector
â”‚ Query Embedding â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [M3] Qdrant     â”‚ Similarity search (cosine)
â”‚ Vector Search   â”‚ Return Top-K chunks (K=5)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [M7] Context    â”‚ Assemble retrieved chunks into context
â”‚ Assembly        â”‚ Add metadata, format for LLM
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [M6] LLM        â”‚ Generate answer using:
â”‚ Generation      â”‚ â€¢ User query
â”‚                 â”‚ â€¢ Retrieved context
â”‚                 â”‚ â€¢ System prompt
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Response        â”‚ Answer + cited sources
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Example:**
```python
# M7: Query script
python scripts/query_rag.py "How to install dependencies?"

# Process:
1. Query â†’ Embedding: [0.1, 0.2, ...]
2. Qdrant search â†’ Top 5 chunks
3. Assemble context from chunks
4. LLM generates answer using context
5. Return answer + sources
```

---

## Key Design Decisions

### 1. Local-First Architecture

**Decision:** Start with local execution, scale later

**Rationale:**
- **Cost:** $0 vs $0.13/1M tokens (OpenAI)
- **Privacy:** Enterprise docs never leave the machine
- **Latency:** No network calls for embeddings
- **Development:** Easier to develop and test locally

**Trade-off:**
- Quality: Local models < API models
- Hardware: Requires decent CPU/RAM
- **Acceptable** for enterprise RAG use case

---

### 2. Flexible LLM Layer (Strategy Pattern)

**Decision:** Abstract LLM providers behind common interface

**Rationale:**
- **No vendor lock-in:** Switch OpenAI â†’ Anthropic â†’ Ollama easily
- **Cost optimization:** Start local, upgrade to API as needed
- **Redundancy:** Fallback if one provider fails
- **A/B testing:** Compare different models easily

**Implementation:**
```python
# Config change only
LLM_PROVIDER=ollama_local  # or openai, anthropic, ollama_server

# Code stays the same
llm = LLMFactory.create()
response = llm.generate(prompt, context)
```

---

### 3. 384-Dimensional Embeddings

**Decision:** Use 384 dims instead of 768 or 1536

**Rationale:**
- **Performance:** 2x faster search than 768 dims
- **Storage:** 2x less RAM/disk than 768 dims
- **Quality:** Sufficient for RAG use case
- **Cost:** If we switch to API, 384 dims cost less

**Comparison:**
| Model | Dims | Quality | Speed | Cost |
|-------|------|---------|-------|------|
| MiniLM-L12 (ours) | 384 | Good | Fast | Free |
| BERT-base | 768 | Better | Medium | Free |
| text-embedding-3-large | 1536 | Best | Slow | $0.13/1M |

---

### 4. Chunking Strategy: 500 tokens + 50 overlap

**Decision:** Fixed-size chunks with small overlap

**Rationale:**
- **500 tokens:** Fits in LLM context window (~1/8 of 4K context)
- **50 token overlap:** Prevents context loss at boundaries
- **Fixed size:** Simpler than semantic chunking
- **Efficient:** Easy to implement and test

**Alternative approaches (not chosen):**
- Semantic chunking (by paragraphs/sections)
  - Pro: Natural boundaries
  - Con: Variable size, complex logic
- Smaller chunks (200 tokens)
  - Pro: More precise retrieval
  - Con: More chunks to search, less context per chunk
- Larger chunks (1000 tokens)
  - Pro: More context per chunk
  - Con: Less precise retrieval

---

### 5. Qdrant for Vector Database

**Decision:** Use Qdrant instead of Pinecone, Weaviate, etc.

**Rationale:**
- **Local-first:** Can run in Docker locally
- **Scalable:** Easy to move to cloud later
- **Performance:** Excellent HNSW implementation
- **Simple API:** Easy to use
- **Open source:** No vendor lock-in

**Alternatives (not chosen):**
- Pinecone: Cloud-only, monthly cost
- Weaviate: More complex setup
- Chroma: Less mature, fewer features
- FAISS: No metadata filtering, manual persistence

---

## Technology Stack

### Core Technologies

| Layer | Technology | Version | Purpose |
|-------|-----------|---------|---------|
| Language | Python | 3.10+ | Modern type hints, async support |
| Config | Pydantic | 2.x | Type-safe configuration |
| Embeddings | sentence-transformers | 5.2+ | Local embedding generation |
| Vector DB | Qdrant | Latest | Vector similarity search |
| PDF Parsing | pypdf | 6.7+ | PDF text and metadata extraction |
| HTML Parsing | BeautifulSoup4 + lxml | 4.14+ | HTML content extraction |
| MD Parsing | python-frontmatter | 1.1+ | Markdown YAML frontmatter |
| LLM (local) | Ollama | Latest | Local LLM inference |
| API | FastAPI | Latest | REST API endpoints |
| Testing | pytest | Latest | Unit and integration tests |

### Development Tools

- **Type checking:** mypy (implicit via Pydantic)
- **Linting:** ruff (planned)
- **Formatting:** black (planned)
- **Documentation:** Markdown + docstrings
- **CI/CD:** GitHub Actions (planned)

---

## Scalability Considerations

### Horizontal Scaling Path

```
Phase 1: Single Machine (Current)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  All-in-One Server         â”‚
â”‚  â€¢ FastAPI                 â”‚
â”‚  â€¢ Qdrant (Docker)         â”‚
â”‚  â€¢ Ollama (Local)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Capacity: ~1000 docs, ~10 req/s


Phase 2: Distributed Components
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Server â”‚  â”‚   Qdrant    â”‚  â”‚   Ollama    â”‚
â”‚  (FastAPI)  â”‚â†’â”‚  (Cloud)    â”‚  â”‚  (Server)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Capacity: ~100K docs, ~100 req/s


Phase 3: Multi-Region
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Load Balancer                   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚         â”‚         â”‚
   â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”
   â”‚ API 1 â”‚ â”‚ API 2â”‚ â”‚ API 3â”‚
   â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Qdrant Cluster â”‚
        â”‚  (Replicated)  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Capacity: ~1M docs, ~1000 req/s
```

### Performance Targets

| Metric | Phase 1 (Current) | Phase 2 | Phase 3 |
|--------|-------------------|---------|---------|
| Documents | 1K | 100K | 1M |
| Queries/sec | 10 | 100 | 1000 |
| Query latency (p95) | 500ms | 200ms | 100ms |
| Ingestion speed | 10 docs/min | 100 docs/min | 1000 docs/min |

### Cost Considerations

**Phase 1 (Local):**
- Hardware: 4GB RAM, 4 CPU cores
- Cost: $0/month (using existing hardware)
- Limits: ~1K documents, ~10 queries/sec

**Phase 2 (Cloud):**
- Qdrant Cloud: ~$50-100/month
- Ollama Server: ~$100-200/month (4 CPU + 16GB RAM)
- API Server: ~$20-50/month
- **Total:** ~$170-350/month
- Capacity: ~100K documents, ~100 queries/sec

**Phase 3 (Enterprise):**
- Qdrant Cluster: ~$500-1000/month
- OpenAI API: ~$500-2000/month (depending on volume)
- Load Balancer + Multi-region: ~$200-500/month
- **Total:** ~$1200-3500/month
- Capacity: ~1M documents, ~1000 queries/sec

---

## Next Steps

**Current Status:** Milestone 4 completed (Document Parsers)

**Next Milestone:** M5 - Ingestion Pipeline (Chunking + Indexing)

See individual milestone documents for detailed implementation plans:
- [Milestone 1: Foundation](milestone-01-foundation.md) âœ…
- [Milestone 2: Embeddings](milestone-02-embeddings.md) âœ…
- [Milestone 3: Vector DB](milestone-03-vector-db.md) âœ…
- [Milestone 4: Parsers](milestone-04-parsers.md) âœ…
- [Milestone 5: Ingestion](milestone-05-ingestion.md) ğŸš§
- [Milestone 6: Flexible LLM](milestone-06-llm.md)
- [Milestone 7: Complete RAG](milestone-07-rag.md)
