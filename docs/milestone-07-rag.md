# Milestone 7: Complete RAG Pipeline

**Status:** ⏸️ Pending
**Dependencies:** M2 (Embeddings), M3 (Vector DB), M5 (Ingestion), M6 (LLM)
**Goal:** Build end-to-end RAG system with API and CLI interfaces

---

## Overview

This milestone integrates all components into a complete RAG (Retrieval-Augmented Generation) system. Users can query documentation through a REST API or interactive CLI, with responses generated by combining vector search and LLM generation.

## RAG Pipeline Flow

```
┌─────────────────────────────────────────────────────────────┐
│                      USER QUERY                             │
│              "How do I configure Qdrant?"                   │
└─────────────────────────────────────────────────────────────┘
                            │
                            ▼
      ┌──────────────────────────────────────────┐
      │  1. QUERY EMBEDDING                      │
      │  - Generate embedding for query          │
      │  - Uses same model as ingestion (M2)     │
      └──────────────────────────────────────────┘
                            │
                            ▼
      ┌──────────────────────────────────────────┐
      │  2. VECTOR SEARCH                        │
      │  - Search Qdrant for similar chunks (M3) │
      │  - Retrieve top-k most relevant          │
      │  - Return chunks + metadata              │
      └──────────────────────────────────────────┘
                            │
                            ▼
      ┌──────────────────────────────────────────┐
      │  3. CONTEXT CONSTRUCTION                 │
      │  - Format retrieved chunks               │
      │  - Add source citations                  │
      │  - Prepare context for LLM               │
      └──────────────────────────────────────────┘
                            │
                            ▼
      ┌──────────────────────────────────────────┐
      │  4. LLM GENERATION                       │
      │  - Send query + context to LLM (M6)      │
      │  - Generate answer                       │
      │  - Stream response (optional)            │
      └──────────────────────────────────────────┘
                            │
                            ▼
      ┌──────────────────────────────────────────┐
      │  5. RESPONSE                             │
      │  - Answer text                           │
      │  - Source citations                      │
      │  - Confidence/similarity scores          │
      └──────────────────────────────────────────┘
```

## Architecture

```
┌──────────────────────────────────────────────────────┐
│                   RAGPipeline                        │
│                                                       │
│  + query(text, top_k, streaming) -> RAGResponse      │
│  + get_sources(query) -> list[Source]                │
└──────────────────────────────────────────────────────┘
                            │
                            │ uses
        ┌──────────────────┼──────────────────┐
        │                  │                  │
        ▼                  ▼                  ▼
┌──────────────┐  ┌─────────────────┐  ┌────────────┐
│ Embedding    │  │ QdrantDatabase  │  │ LLMProvider│
│ Service (M2) │  │ (M3)            │  │ (M6)       │
└──────────────┘  └─────────────────┘  └────────────┘


┌──────────────────────────────────────────────────────┐
│                   FastAPI Server                     │
│                                                       │
│  POST /query          - RAG query endpoint           │
│  POST /query/stream   - Streaming RAG response       │
│  GET  /sources        - List indexed documents       │
│  GET  /health         - Health check                 │
└──────────────────────────────────────────────────────┘


┌──────────────────────────────────────────────────────┐
│                 Interactive CLI                      │
│                                                       │
│  - REPL interface                                    │
│  - Streaming responses                               │
│  - Source citations                                  │
│  - Multi-project support                             │
└──────────────────────────────────────────────────────┘
```

## Implementation Plan

### Task 1: RAG Data Models

**File:** `src/rag/models.py`

```python
"""
Data models for RAG pipeline.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Optional


@dataclass
class Source:
    """
    Represents a source document chunk retrieved from vector database.
    """
    chunk_text: str
    source_file: str
    chunk_index: int
    similarity_score: float
    document_title: Optional[str] = None
    document_format: Optional[str] = None


@dataclass
class RAGResponse:
    """
    Response from RAG pipeline.
    """
    # Generated answer
    answer: str

    # Retrieved sources
    sources: list[Source]

    # Metadata
    query: str
    model_used: str
    retrieval_count: int

    # Optional timing info
    retrieval_time_ms: Optional[float] = None
    generation_time_ms: Optional[float] = None


@dataclass
class RAGConfig:
    """
    Configuration for RAG pipeline.
    """
    top_k: int = 5              # Number of chunks to retrieve
    temperature: float = 0.7     # LLM temperature
    max_tokens: int = 1024       # Max LLM response tokens
    min_similarity: float = 0.3  # Minimum similarity threshold
    streaming: bool = False      # Enable streaming responses
```

### Task 2: RAG Pipeline Core

**File:** `src/rag/pipeline.py`

```python
"""
Main RAG (Retrieval-Augmented Generation) pipeline.
"""

from __future__ import annotations

import logging
import time
from typing import Iterator, Optional

from src.database import QdrantDatabase
from src.embeddings import EmbeddingService
from src.llm import LLMProviderFactory

from .models import RAGConfig, RAGResponse, Source


logger = logging.getLogger(__name__)


class RAGPipeline:
    """
    End-to-end RAG pipeline.

    Combines vector search and LLM generation to answer queries
    based on indexed documentation.
    """

    def __init__(
        self,
        embedding_service: Optional[EmbeddingService] = None,
        vector_db: Optional[QdrantDatabase] = None,
        llm_provider: Optional[str] = None,
        config: Optional[RAGConfig] = None,
    ) -> None:
        """
        Initialize RAG pipeline.

        Args:
            embedding_service: Service for generating embeddings
            vector_db: Vector database for retrieval
            llm_provider: LLM provider type (or from env)
            config: RAG configuration
        """
        self.embedding_service = embedding_service or EmbeddingService()
        self.vector_db = vector_db or QdrantDatabase()

        # Create LLM provider via factory
        from src.llm import LLMProviderFactory
        self.llm = LLMProviderFactory.create_provider(llm_provider)

        self.config = config or RAGConfig()

        logger.info("RAG pipeline initialized")
        logger.info(f"LLM: {self.llm.get_model_info()}")

    def query(
        self,
        query_text: str,
        top_k: Optional[int] = None,
        streaming: bool = False,
    ) -> RAGResponse | Iterator[str]:
        """
        Execute RAG query.

        Args:
            query_text: User query
            top_k: Number of chunks to retrieve (overrides config)
            streaming: Enable streaming response

        Returns:
            RAGResponse with answer and sources, or streaming iterator
        """
        if not query_text or not query_text.strip():
            raise ValueError("Query cannot be empty")

        top_k = top_k or self.config.top_k

        logger.info(f"Processing query: {query_text[:100]}...")

        # 1. Generate query embedding
        start_retrieval = time.time()

        query_embedding = self.embedding_service.generate_embedding(query_text)

        # 2. Retrieve similar chunks from vector DB
        search_results = self.vector_db.search_similar(
            query_vector=query_embedding,
            limit=top_k,
        )

        retrieval_time = (time.time() - start_retrieval) * 1000  # ms

        logger.info(f"Retrieved {len(search_results)} chunks in {retrieval_time:.2f}ms")

        # 3. Filter by minimum similarity
        filtered_results = [
            r for r in search_results
            if r.get('score', 0) >= self.config.min_similarity
        ]

        if not filtered_results:
            logger.warning("No results above similarity threshold")
            return RAGResponse(
                answer="I couldn't find relevant information to answer your question.",
                sources=[],
                query=query_text,
                model_used=self.llm.model,
                retrieval_count=0,
                retrieval_time_ms=retrieval_time,
            )

        # 4. Construct sources
        sources = [
            Source(
                chunk_text=r['metadata']['chunk_text'],
                source_file=r['metadata']['source_file'],
                chunk_index=r['metadata']['chunk_index'],
                similarity_score=r['score'],
                document_title=r['metadata'].get('document_title'),
                document_format=r['metadata'].get('document_format'),
            )
            for r in filtered_results
        ]

        # 5. Build context from sources
        context = self._build_context(sources)

        # 6. Generate answer with LLM
        start_generation = time.time()

        if streaming:
            # Return streaming generator
            return self._generate_streaming(query_text, context, sources)
        else:
            # Return complete response
            answer = self.llm.generate(
                prompt=query_text,
                context=context,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
            )

            generation_time = (time.time() - start_generation) * 1000  # ms

            logger.info(f"Generated answer in {generation_time:.2f}ms")

            return RAGResponse(
                answer=answer,
                sources=sources,
                query=query_text,
                model_used=self.llm.model,
                retrieval_count=len(sources),
                retrieval_time_ms=retrieval_time,
                generation_time_ms=generation_time,
            )

    def _build_context(self, sources: list[Source]) -> str:
        """
        Build context string from retrieved sources.

        Args:
            sources: Retrieved source chunks

        Returns:
            Formatted context string
        """
        context_parts = []

        for i, source in enumerate(sources, 1):
            # Include source file and similarity score
            header = f"[Source {i}] {source.source_file} (similarity: {source.similarity_score:.2f})"
            context_parts.append(f"{header}\n{source.chunk_text}\n")

        return "\n".join(context_parts)

    def _generate_streaming(
        self,
        query: str,
        context: str,
        sources: list[Source]
    ) -> Iterator[str]:
        """
        Generate streaming response.

        Yields:
            Chunks of generated text
        """
        try:
            for chunk in self.llm.generate_stream(
                prompt=query,
                context=context,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
            ):
                yield chunk

        except Exception as e:
            logger.error(f"Streaming generation failed: {e}")
            raise

    def get_indexed_sources(self) -> list[dict]:
        """
        Get list of all indexed documents.

        Returns:
            List of document metadata
        """
        # Get collection info from vector DB
        info = self.vector_db.get_collection_info()
        return info
```

### Task 3: FastAPI Server

**File:** `src/api/server.py`

```python
"""
FastAPI server for RAG system.
"""

from __future__ import annotations

import logging
from contextlib import asynccontextmanager
from typing import Optional

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

from src.rag import RAGPipeline, RAGConfig


logger = logging.getLogger(__name__)

# Global RAG pipeline instance
rag_pipeline: Optional[RAGPipeline] = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Initialize and cleanup RAG pipeline."""
    global rag_pipeline

    logger.info("Initializing RAG pipeline...")
    rag_pipeline = RAGPipeline()
    logger.info("RAG pipeline ready")

    yield

    logger.info("Shutting down RAG pipeline")


app = FastAPI(
    title="DocVault API",
    description="RAG-powered documentation query system",
    version="1.0.0",
    lifespan=lifespan,
)


# Request/Response models
class QueryRequest(BaseModel):
    """Request model for RAG query."""
    query: str
    top_k: int = 5
    temperature: float = 0.7
    max_tokens: int = 1024


class SourceResponse(BaseModel):
    """Response model for source chunk."""
    chunk_text: str
    source_file: str
    chunk_index: int
    similarity_score: float
    document_title: Optional[str] = None


class QueryResponse(BaseModel):
    """Response model for RAG query."""
    answer: str
    sources: list[SourceResponse]
    query: str
    model_used: str
    retrieval_count: int
    retrieval_time_ms: Optional[float] = None
    generation_time_ms: Optional[float] = None


# Endpoints
@app.get("/health")
async def health_check():
    """Health check endpoint."""
    return {
        "status": "healthy",
        "pipeline_initialized": rag_pipeline is not None
    }


@app.post("/query", response_model=QueryResponse)
async def query(request: QueryRequest):
    """
    Execute RAG query.

    Args:
        request: Query request with parameters

    Returns:
        Generated answer with sources
    """
    if not rag_pipeline:
        raise HTTPException(status_code=503, detail="Pipeline not initialized")

    try:
        # Update config
        rag_pipeline.config.top_k = request.top_k
        rag_pipeline.config.temperature = request.temperature
        rag_pipeline.config.max_tokens = request.max_tokens

        # Execute query
        response = rag_pipeline.query(request.query, streaming=False)

        # Convert to API response
        return QueryResponse(
            answer=response.answer,
            sources=[
                SourceResponse(
                    chunk_text=s.chunk_text,
                    source_file=s.source_file,
                    chunk_index=s.chunk_index,
                    similarity_score=s.similarity_score,
                    document_title=s.document_title,
                )
                for s in response.sources
            ],
            query=response.query,
            model_used=response.model_used,
            retrieval_count=response.retrieval_count,
            retrieval_time_ms=response.retrieval_time_ms,
            generation_time_ms=response.generation_time_ms,
        )

    except Exception as e:
        logger.error(f"Query failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/query/stream")
async def query_stream(request: QueryRequest):
    """
    Execute RAG query with streaming response.

    Args:
        request: Query request with parameters

    Returns:
        Streaming text response
    """
    if not rag_pipeline:
        raise HTTPException(status_code=503, detail="Pipeline not initialized")

    try:
        # Update config
        rag_pipeline.config.top_k = request.top_k
        rag_pipeline.config.temperature = request.temperature
        rag_pipeline.config.max_tokens = request.max_tokens

        # Execute streaming query
        stream = rag_pipeline.query(request.query, streaming=True)

        return StreamingResponse(
            stream,
            media_type="text/plain"
        )

    except Exception as e:
        logger.error(f"Streaming query failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/sources")
async def list_sources():
    """
    Get list of indexed documents.

    Returns:
        List of document metadata
    """
    if not rag_pipeline:
        raise HTTPException(status_code=503, detail="Pipeline not initialized")

    try:
        sources = rag_pipeline.get_indexed_sources()
        return {"sources": sources}

    except Exception as e:
        logger.error(f"Failed to list sources: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# Run server
if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "src.api.server:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
```

### Task 4: Interactive CLI

**File:** `src/cli/interactive.py`

```python
"""
Interactive CLI for DocVault RAG system.
"""

from __future__ import annotations

import sys
from typing import Optional

from rich.console import Console
from rich.markdown import Markdown
from rich.panel import Panel
from rich.prompt import Prompt

from src.rag import RAGPipeline


console = Console()


class InteractiveCLI:
    """
    Interactive command-line interface for RAG queries.
    """

    def __init__(self, rag_pipeline: Optional[RAGPipeline] = None) -> None:
        """
        Initialize CLI.

        Args:
            rag_pipeline: RAG pipeline instance
        """
        self.rag = rag_pipeline or RAGPipeline()

        # Get model info
        model_info = self.rag.llm.get_model_info()
        self.model_display = f"{model_info['provider']}/{model_info['model']}"

    def print_banner(self) -> None:
        """Print welcome banner."""
        banner = """
# DocVault - RAG Documentation Assistant

Ask questions about your indexed documentation.

**Commands:**
- Type your question and press Enter
- `/sources` - List indexed documents
- `/help` - Show help
- `/exit` - Exit

**Model:** {model}
        """.format(model=self.model_display)

        console.print(Markdown(banner))
        console.print()

    def run(self) -> None:
        """Run interactive REPL loop."""
        self.print_banner()

        while True:
            try:
                # Get user input
                query = Prompt.ask("\n[bold cyan]❯[/bold cyan]")

                if not query or not query.strip():
                    continue

                # Handle commands
                if query.startswith("/"):
                    self.handle_command(query)
                    continue

                # Execute RAG query
                self.execute_query(query)

            except KeyboardInterrupt:
                console.print("\n\n[yellow]Use /exit to quit[/yellow]")
                continue
            except EOFError:
                break

        console.print("\n[green]Goodbye![/green]")

    def execute_query(self, query: str) -> None:
        """Execute RAG query and display results."""
        console.print("\n[dim]Searching documentation...[/dim]")

        try:
            # Execute query
            response = self.rag.query(query, streaming=False)

            # Display answer
            console.print("\n")
            console.print(Panel(
                Markdown(response.answer),
                title="[bold green]Answer[/bold green]",
                border_style="green"
            ))

            # Display sources
            if response.sources:
                console.print(f"\n[dim]Sources ({len(response.sources)}):[/dim]")
                for i, source in enumerate(response.sources, 1):
                    console.print(
                        f"  [{i}] {source.source_file} "
                        f"(similarity: {source.similarity_score:.2f})"
                    )

            # Display timing
            if response.retrieval_time_ms and response.generation_time_ms:
                total_time = response.retrieval_time_ms + response.generation_time_ms
                console.print(
                    f"\n[dim]⏱️  {total_time:.0f}ms "
                    f"(retrieval: {response.retrieval_time_ms:.0f}ms, "
                    f"generation: {response.generation_time_ms:.0f}ms)[/dim]"
                )

        except Exception as e:
            console.print(f"\n[red]Error: {e}[/red]")

    def handle_command(self, command: str) -> None:
        """Handle CLI commands."""
        command = command.lower().strip()

        if command == "/exit" or command == "/quit":
            raise EOFError

        elif command == "/help":
            self.print_banner()

        elif command == "/sources":
            self.show_sources()

        else:
            console.print(f"[red]Unknown command: {command}[/red]")

    def show_sources(self) -> None:
        """Display indexed sources."""
        console.print("\n[bold]Indexed Documents:[/bold]")

        try:
            info = self.rag.get_indexed_sources()
            console.print(f"  Total vectors: {info.get('vectors_count', 0)}")
            console.print(f"  Collection: {info.get('collection_name', 'unknown')}")

        except Exception as e:
            console.print(f"[red]Error: {e}[/red]")


def main() -> None:
    """Run interactive CLI."""
    # Set UTF-8 encoding for Windows
    if sys.platform == "win32":
        import io
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

    try:
        cli = InteractiveCLI()
        cli.run()
    except Exception as e:
        console.print(f"\n[red]Fatal error: {e}[/red]")
        sys.exit(1)


if __name__ == "__main__":
    main()
```

### Task 5: Testing

**File:** `tests/test_rag_pipeline.py`

Test coverage:
- End-to-end RAG flow
- Mocked LLM responses
- Edge cases (no results, empty query)
- Streaming vs non-streaming

**File:** `tests/test_api.py`

API endpoint tests:
- Health check
- Query endpoint
- Streaming endpoint
- Error handling

### Task 6: Documentation

Update README.md with:
- Quick start guide
- API usage examples
- CLI usage examples
- Configuration guide

### Task 7: Docker Support (Optional)

**File:** `Dockerfile`

```dockerfile
FROM python:3.10-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy source code
COPY . .

# Expose API port
EXPOSE 8000

# Run API server
CMD ["python", "-m", "src.api.server"]
```

**File:** `docker-compose.yml`

```yaml
version: '3.8'

services:
  docvault:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
    environment:
      - LLM_PROVIDER=ollama_server
      - LLM_SERVER_URL=http://ollama:11434
    depends_on:
      - ollama

  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama

volumes:
  ollama_data:
```

## Usage Examples

### Python API
```python
from src.rag import RAGPipeline

# Initialize pipeline
rag = RAGPipeline()

# Query
response = rag.query("How do I install Docker?")

print(response.answer)
print(f"Sources: {len(response.sources)}")
```

### REST API
```bash
# Start server
python -m src.api.server

# Query via curl
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "How do I install Docker?",
    "top_k": 5
  }'

# Streaming query
curl -X POST http://localhost:8000/query/stream \
  -H "Content-Type: application/json" \
  -d '{"query": "What is Kubernetes?"}' \
  --no-buffer
```

### Interactive CLI
```bash
python -m src.cli.interactive

# Interactive session
❯ How do I configure logging?

Answer:
To configure logging in DocVault, you can set the LOG_LEVEL
environment variable...

Sources (3):
  [1] docs/configuration.md (similarity: 0.89)
  [2] docs/troubleshooting.md (similarity: 0.76)
  [3] README.md (similarity: 0.71)

⏱️  324ms (retrieval: 45ms, generation: 279ms)

❯ /exit
```

## Performance Benchmarks

### Query Latency
- **Retrieval**: 20-50ms (vector search)
- **Generation**: 200-2000ms (depends on LLM)
- **Total**: 220-2050ms

### Throughput
- **Sequential**: ~1-5 queries/second
- **Parallel (future)**: ~10-20 queries/second

### Accuracy
- **Retrieval Accuracy**: 85-95% (relevant chunks in top-5)
- **Answer Quality**: Depends on LLM model

## Monitoring and Logging

### Metrics to Track
- Query latency (p50, p95, p99)
- Retrieval count per query
- LLM token usage
- Error rate

### Logging
```
2026-02-11 10:23:45 - INFO - Processing query: How do I...
2026-02-11 10:23:45 - INFO - Retrieved 5 chunks in 42.3ms
2026-02-11 10:23:46 - INFO - Generated answer in 287.1ms
```

## Verification Criteria

**M7 is complete when:**
- [ ] RAG pipeline integrates all components
- [ ] FastAPI server runs and responds to queries
- [ ] Interactive CLI provides REPL interface
- [ ] Streaming responses work
- [ ] Source citations are included
- [ ] All unit tests pass
- [ ] End-to-end integration tests pass
- [ ] Documentation is comprehensive
- [ ] Docker deployment works (optional)

## Future Enhancements

### Short-term
- Conversation history/memory
- Multi-query support
- Better prompt engineering
- Result caching

### Long-term
- Multi-project support
- Query rewriting
- Hybrid search (vector + keyword)
- Fine-tuned embedding models
- User feedback loop

---

**Related Files:**
- `src/rag/models.py` - RAG data models
- `src/rag/pipeline.py` - RAG pipeline core
- `src/api/server.py` - FastAPI server
- `src/cli/interactive.py` - Interactive CLI
- `tests/test_rag_pipeline.py` - Pipeline tests
- `tests/test_api.py` - API tests
- `Dockerfile` - Docker container
- `docker-compose.yml` - Docker Compose config
