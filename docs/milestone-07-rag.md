# Milestone 7: Complete RAG Pipeline

**Status:** ✅ Done
**Dependencies:** M2 (Embeddings), M3 (Vector DB), M5 (Ingestion), M6 (LLM)
**Goal:** Build end-to-end RAG system with API and CLI interfaces

---

## Overview

This milestone integrates all previous components into a complete RAG (Retrieval-Augmented Generation) system. Users can query documentation through a REST API or interactive CLI, with responses generated by combining vector search and LLM generation.

**Key result:** Ingest documents → ask questions → get AI-generated answers with source citations.

## Architecture

```
┌──────────────────────────────────────────────────────┐
│                   RAGPipeline                        │
│                                                       │
│  + query(text, top_k, temperature, max_tokens,       │
│          streaming) -> RAGResponse | Iterator[str]   │
│  + get_indexed_sources() -> dict                     │
└──────────────────────────────────────────────────────┘
                        │
                        │ uses (DI)
        ┌───────────────┼───────────────┐
        │               │               │
        ▼               ▼               ▼
┌──────────────┐ ┌─────────────┐ ┌────────────┐
│ Embedding    │ │ Qdrant      │ │ LLMProvider│
│ Service (M2) │ │ Database(M3)│ │ (M6)       │
└──────────────┘ └─────────────┘ └────────────┘


┌──────────────────────────────────────────────────────┐
│                   FastAPI Server                     │
│                                                       │
│  GET  /health         - Pipeline health status       │
│  POST /query          - RAG query → JSON response    │
│  POST /query/stream   - RAG query → streaming text   │
│  GET  /sources        - Indexed collection info      │
└──────────────────────────────────────────────────────┘


┌──────────────────────────────────────────────────────┐
│                 Interactive CLI                      │
│                                                       │
│  REPL interface with rich formatting                 │
│  Commands: /sources, /help, /exit                    │
│  Score-colored source citations                      │
│  Timing breakdown (retrieval + generation)           │
└──────────────────────────────────────────────────────┘
```

## Implementation

### Files Created

| File | Purpose | Lines |
|------|---------|-------|
| `src/rag/config.py` | RAG constants (DEFAULT_TOP_K, MIN_SIMILARITY, NO_RESULTS_MESSAGE) | 22 |
| `src/rag/models.py` | Source, RAGResponse, RAGConfig dataclasses | 44 |
| `src/rag/pipeline.py` | RAGPipeline — main orchestrator | 204 |
| `src/rag/__init__.py` | Module exports | 16 |
| `src/api/__init__.py` | API module docstring | 6 |
| `src/api/server.py` | FastAPI server with 4 endpoints | 223 |
| `src/cli/__init__.py` | CLI module docstring | 6 |
| `src/cli/interactive.py` | Interactive CLI with rich formatting | 182 |

### Files Modified

| File | Changes |
|------|---------|
| `config/settings.py` | Added 4 fields: rag_top_k, rag_min_similarity, api_host, api_port |
| `.env.example` | Added RAG and API configuration variables |

### Key Components

#### 1. RAGPipeline (`pipeline.py`)

The central orchestrator. Uses dependency injection for all services:

```python
pipeline = RAGPipeline(
    embedding_service=EmbeddingService(),    # or mock
    vector_db=QdrantDatabase(in_memory=True), # or mock
    llm_provider=mock_llm,                   # or real provider
    config=RAGConfig(top_k=5),
)
```

**Query flow (5 steps):**

1. **Validate** query text (non-empty)
2. **Embed** query → 384-dim vector via EmbeddingService
3. **Search** vector DB with score_threshold → list of dicts
4. **Build context** from sources with headers: `[Source N] file.md (similarity: 0.92)`
5. **Generate** answer via LLM (sync or streaming)

**Key design decisions vs spec:**
- `llm_provider` accepts `LLMProvider` instance (not string) — enables direct DI
- Parameters `top_k`, `temperature`, `max_tokens` passed per-call, not mutating config
- `streaming` is a per-call parameter, not in RAGConfig
- Uses `time.perf_counter()` (not `time.time()`) for precise timing
- Logger uses `%s` formatting (not f-strings)

#### 2. Data Models (`models.py`)

Three dataclasses:

- **Source:** chunk_text, source_file, chunk_index, similarity_score, document_title?, document_format?
- **RAGResponse:** answer, sources, query, model_used, retrieval_count, retrieval_time_ms?, generation_time_ms?
- **RAGConfig:** top_k=5, temperature=0.7, max_tokens=1024, min_similarity=0.3

#### 3. FastAPI Server (`server.py`)

**Pydantic models** (BaseModel, not dataclass) for request/response with Field validation:

```python
class QueryRequest(BaseModel):
    query: str = Field(..., min_length=1)
    top_k: Optional[int] = Field(default=None, ge=1, le=20)
    temperature: Optional[float] = Field(default=None, ge=0.0, le=2.0)
    max_tokens: Optional[int] = Field(default=None, ge=1, le=4096)
```

**Lifespan** with `@asynccontextmanager`: creates RAGPipeline on startup, catches errors gracefully (sets pipeline to None if init fails).

**Endpoints are sync** (`def`, not `async def`) because all underlying operations (embedding, search, LLM) are synchronous.

**Error codes:**
- 503 Service Unavailable — pipeline not initialized
- 500 Internal Server Error — pipeline operation failed

#### 4. Interactive CLI (`interactive.py`)

Uses `rich` library for terminal formatting:
- **Panel** for answer display (green border)
- **Markdown** rendering for LLM responses
- **Color-coded** similarity scores (green > 0.7, yellow otherwise)
- **Lazy initialization** — pipeline created on first use, not in constructor

Commands: `/sources` (collection info panel), `/help` (banner), `/exit`/`/quit`.

### Configuration (`config/settings.py`)

Added fields:
```python
# RAG Pipeline
rag_top_k: int = Field(default=5)
rag_min_similarity: float = Field(default=0.3)

# API Server
api_host: str = Field(default="0.0.0.0")
api_port: int = Field(default=8000)
```

## Testing

### Unit Tests — RAG Pipeline (15 tests)

| Test Class | Tests | What it tests |
|-----------|-------|---------------|
| `TestRAGConfig` | 2 | Default values, custom values |
| `TestRAGModels` | 3 | Source creation, RAGResponse with sources, empty sources |
| `TestRAGPipeline` | 10 | Full query flow, embedding call, DB search, LLM context, empty query, no results, custom params, streaming, context format, indexed sources |

All pipeline tests use `MagicMock` fixtures for embedding service, vector DB, and LLM provider.

### Unit Tests — API (9 tests)

| Test Class | Tests | What it tests |
|-----------|-------|---------------|
| `TestHealthEndpoint` | 2 | Health with/without pipeline |
| `TestQueryEndpoint` | 3 | Success, 503 no pipeline, 500 error |
| `TestStreamEndpoint` | 2 | Streaming success, 503 no pipeline |
| `TestSourcesEndpoint` | 2 | Sources success, 503 no pipeline |

API tests patch `RAGPipeline` constructor (not the global variable) to control lifespan behavior. Constructor raises `RuntimeError` for "no pipeline" tests.

### Integration Tests (4 tests)

| Test | What it verifies |
|------|--------------------|
| `test_query_returns_relevant_sources` | Real embeddings + Qdrant find "docker" for Docker query |
| `test_query_empty_database` | Empty DB returns NO_RESULTS_MESSAGE |
| `test_context_contains_chunk_text` | LLM receives context with chunk text and [Source] headers |
| `test_streaming_with_real_retrieval` | Streaming works with real retrieval + mock LLM |

Integration tests use real `EmbeddingService` (scope=class) and in-memory `QdrantDatabase` with mock LLM.

### Running Tests

```bash
# Unit tests only (fast, no models needed)
pytest tests/unit/test_rag.py tests/unit/test_api.py -v

# Integration tests (loads embedding model)
pytest tests/integration/test_rag_integration.py -v

# All M7 tests
pytest tests/ -k "rag or api" -v

# All project tests
pytest tests/ -v
```

**Results:** 28 M7 tests passed (15 RAG + 9 API + 4 integration). 185 total project tests passed, 3 skipped (Ollama).

## Usage Examples

### Python API

```python
from src.rag import RAGPipeline

# Create pipeline from .env configuration
pipeline = RAGPipeline()

# Sync query
response = pipeline.query("How do I install Docker?")
print(response.answer)
for source in response.sources:
    print(f"  {source.source_file} (score: {source.similarity_score:.2f})")

# Streaming query
for chunk in pipeline.query("Explain RAG", streaming=True):
    print(chunk, end="")
```

### REST API

```bash
# Start server
python -m src.api.server

# Health check
curl http://localhost:8000/health

# Query
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"query": "How do I install Docker?", "top_k": 5}'

# Streaming query
curl -X POST http://localhost:8000/query/stream \
  -H "Content-Type: application/json" \
  -d '{"query": "What is Kubernetes?"}' \
  --no-buffer

# Indexed sources
curl http://localhost:8000/sources
```

### Interactive CLI

```bash
python -m src.cli.interactive

# Session example:
> How do I configure logging?

┌─ Answer ────────────────────────────────────────┐
│ To configure logging in DocVault, set the       │
│ LOG_LEVEL environment variable in your .env...  │
└─────────────────────────────────────────────────┘

Sources (3 chunks):
  1. docs/configuration.md (score: 0.89)
  2. docs/troubleshooting.md (score: 0.76)
  3. README.md (score: 0.71)
retrieval: 45ms | generation: 279ms | model: llama3.2:3b

> /sources
┌─ Indexed Sources ───────────────────────────────┐
│ Collection: docvault_documents                   │
│ Vectors:    1247                                 │
│ Dimensions: 384                                  │
│ Metric:     Cosine                               │
│ Status:     green                                │
└─────────────────────────────────────────────────┘

> /exit
```

## Differences from Original Spec

| Aspect | Original doc | Actual implementation |
|--------|-------------|----------------------|
| `llm_provider` param | String type | `LLMProvider` instance (direct DI) |
| Config mutation | `rag_pipeline.config.top_k = request.top_k` | Parameters passed per-call (thread-safe) |
| `streaming` | In RAGConfig dataclass | Per-call parameter (not in config) |
| Endpoint type | `async def` | `def` (sync, operations are synchronous) |
| Lifespan | No error handling | try/except with graceful degradation |
| CLI initialization | Eager in constructor | Lazy via `_ensure_pipeline()` |
| Timer | `time.time()` | `time.perf_counter()` (higher precision) |
| Logger | f-strings | `%s` formatting (best practice) |
| Docker | Dockerfile + docker-compose | Not created (optional in spec) |
| Test location | `tests/test_rag_pipeline.py` + `tests/test_api.py` | `tests/unit/test_rag.py` + `tests/unit/test_api.py` + `tests/integration/test_rag_integration.py` |

## Complete Project Summary

With M7, all milestones are complete. The full pipeline:

```
Documents (PDF/HTML/MD)
    → Parse (M4)
    → Chunk (M5)
    → Embed (M2)
    → Store in Qdrant (M3)

User Question
    → Embed query (M2)
    → Search Qdrant (M3)
    → Build context with citations
    → Generate answer via LLM (M6)
    → Return RAGResponse with sources

Interfaces:
    → REST API (FastAPI + uvicorn)
    → Interactive CLI (rich)
    → Python API (direct import)
```

**Total tests: 185 passed, 3 skipped.**

---

**Related Files:**
- `src/rag/config.py` — RAG constants
- `src/rag/models.py` — Source, RAGResponse, RAGConfig
- `src/rag/pipeline.py` — RAGPipeline orchestrator
- `src/rag/__init__.py` — Module exports
- `src/api/server.py` — FastAPI server
- `src/cli/interactive.py` — Interactive CLI
- `tests/unit/test_rag.py` — 15 unit tests
- `tests/unit/test_api.py` — 9 API tests
- `tests/integration/test_rag_integration.py` — 4 integration tests
